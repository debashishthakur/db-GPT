{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "1acef9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "215f308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_vocab_size = 114 # Vocabulary size for the source language\n",
    "output_vocab_size = 80 # Vocabulary size for the target language\n",
    "embedding_dim = 100  # Dimension of the GloVe embeddings\n",
    "hidden_units = 32 # Number of units in LSTM layers\n",
    "sequence_length = 35 # Maximum sequence length\n",
    "batch_size = 32 # Batch size\n",
    "num_epochs = 150 # Number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "32dd3922",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = 'GloVe/glove.6B.100d.txt'  # Adjust the path to your downloaded GloVe file\n",
    "embedding_matrix = {}  # Create an empty dictionary to store the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "4be9cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(glove_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_matrix[word] = coefs #(coeff == Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "3c3cb27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix for the source and target languages\n",
    "source_embedding_matrix = np.zeros((input_vocab_size, embedding_dim))\n",
    "target_embedding_matrix = np.zeros((output_vocab_size, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "124299d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the dataset\n",
    "with open('card.json', 'r') as json_file:\n",
    "    dataset = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e9d12b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping input and output sequences to integers \n",
    "#input sequence = source_tokenizer\n",
    "#output sequence = target_tokenizer\n",
    "\n",
    "index_s = 1\n",
    "index_t = 1\n",
    "source_tokenizer = {}\n",
    "target_tokenizer = {}\n",
    "\n",
    "\n",
    "for i in dataset:\n",
    "    question_toks = i[\"question_toks\"]\n",
    "    # print(query_toks)\n",
    "    for j in question_toks:\n",
    "        # print(j)\n",
    "        \n",
    "        if j not in source_tokenizer:\n",
    "            source_tokenizer[j] = index_s\n",
    "            index_s += 1\n",
    "# print((frequency))\n",
    "for i in dataset:\n",
    "    query_toks = i[\"query_toks\"]\n",
    "    # print(query_toks)\n",
    "    for j in query_toks:\n",
    "        # print(j)\n",
    "        \n",
    "        if j not in target_tokenizer:\n",
    "            target_tokenizer[j] = index_t\n",
    "            index_t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "579a7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in source_tokenizer.items():\n",
    "    embedding_vector = embedding_matrix.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        source_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "for word, i in target_tokenizer.items():\n",
    "    embedding_vector = embedding_matrix.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        target_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "460cc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tokens = [example[\"question_toks\"] for example in dataset]\n",
    "query_tokens = [example[\"query_toks\"] for example in dataset]\n",
    "\n",
    "encoder_input_data_nopad = [[source_tokenizer[word] for word in sentence] for sentence in question_tokens]\n",
    "\n",
    "\n",
    "decoder_input_data_nopad = [[target_tokenizer[word] for word in sentence] for sentence in query_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3b029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2e4de594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences to a consistent length\n",
    "def pad_sequences(sequences, max_length):\n",
    "    padded_sequences = []\n",
    "    for sequence in sequences:\n",
    "        if len(sequence) < max_length:\n",
    "            padded_sequence = sequence + [0] * (max_length - len(sequence))\n",
    "        else:\n",
    "            padded_sequence = sequence[:max_length]\n",
    "        padded_sequences.append(padded_sequence)\n",
    "    return padded_sequences\n",
    "\n",
    "# Pad the encoder and decoder inputs\n",
    "encoder_input_data = pad_sequences(encoder_input_data_nopad, sequence_length)\n",
    "decoder_input_data = pad_sequences(decoder_input_data_nopad, sequence_length)\n",
    "encoder_input_data = np.array(encoder_input_data)\n",
    "decoder_input_data = np.array(decoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "83607459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3, ...,  0,  0,  0],\n",
       "       [ 1,  2,  3, ...,  0,  0,  0],\n",
       "       [ 1,  8,  9, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 1, 71,  6, ...,  0,  0,  0],\n",
       "       [ 1,  8,  9, ...,  0,  0,  0],\n",
       "       [ 1,  8,  9, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d020d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "62ac1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder_target_data\n",
    "target_vocab = {\n",
    "    0 : 0\n",
    "}\n",
    "\n",
    "decoder_target_data = np.zeros_like(decoder_input_data)\n",
    "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]\n",
    "decoder_target_data[:, -1] = target_vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "efae0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(sequence_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, embedding_dim, weights=[source_embedding_matrix], trainable=False)(encoder_inputs)\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d1d267bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(sequence_length,))\n",
    "decoder_embedding = Embedding(output_vocab_size, embedding_dim, weights=[target_embedding_matrix], trainable=False)(decoder_inputs)\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "output = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2e4e07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = Model([encoder_inputs, decoder_inputs], output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "899d6ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)       [(None, 35)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)       [(None, 35)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)    (None, 35, 100)              11400     ['input_13[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_13 (Embedding)    (None, 35, 100)              8000      ['input_14[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_12 (LSTM)              [(None, 32),                 17024     ['embedding_12[0][0]']        \n",
      "                              (None, 32),                                                         \n",
      "                              (None, 32)]                                                         \n",
      "                                                                                                  \n",
      " lstm_13 (LSTM)              [(None, 35, 32),             17024     ['embedding_13[0][0]',        \n",
      "                              (None, 32),                            'lstm_12[0][1]',             \n",
      "                              (None, 32)]                            'lstm_12[0][2]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 35, 80)               2640      ['lstm_13[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 56088 (219.09 KB)\n",
      "Trainable params: 36688 (143.31 KB)\n",
      "Non-trainable params: 19400 (75.78 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "87c9e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 35)\n",
      "(80, 35)\n",
      "(80, 35)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_target_data.shape)\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "7ee10b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2/2 [==============================] - 6s 1s/step - loss: 4.3718 - val_loss: 4.3595\n",
      "Epoch 2/150\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 4.3425 - val_loss: 4.3365\n",
      "Epoch 3/150\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 4.3161 - val_loss: 4.3133\n",
      "Epoch 4/150\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 4.2883 - val_loss: 4.2879\n",
      "Epoch 5/150\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 4.2574 - val_loss: 4.2587\n",
      "Epoch 6/150\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 4.2227 - val_loss: 4.2238\n",
      "Epoch 7/150\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 4.1823 - val_loss: 4.1806\n",
      "Epoch 8/150\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 4.1330 - val_loss: 4.1260\n",
      "Epoch 9/150\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 4.0730 - val_loss: 4.0554\n",
      "Epoch 10/150\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 4.0002 - val_loss: 3.9630\n",
      "Epoch 11/150\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.9059 - val_loss: 3.8428\n",
      "Epoch 12/150\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.7967 - val_loss: 3.6910\n",
      "Epoch 13/150\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.6600 - val_loss: 3.5114\n",
      "Epoch 14/150\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.5156 - val_loss: 3.3144\n",
      "Epoch 15/150\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.3637 - val_loss: 3.1138\n",
      "Epoch 16/150\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3.2199 - val_loss: 2.9276\n",
      "Epoch 17/150\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.0949 - val_loss: 2.7648\n",
      "Epoch 18/150\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 2.9806 - val_loss: 2.6226\n",
      "Epoch 19/150\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 2.8749 - val_loss: 2.4945\n",
      "Epoch 20/150\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 2.7788 - val_loss: 2.3758\n",
      "Epoch 21/150\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 2.6892 - val_loss: 2.2643\n",
      "Epoch 22/150\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.6067 - val_loss: 2.1612\n",
      "Epoch 23/150\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 2.5301 - val_loss: 2.0664\n",
      "Epoch 24/150\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.4588 - val_loss: 1.9798\n",
      "Epoch 25/150\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 2.3971 - val_loss: 1.9005\n",
      "Epoch 26/150\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 2.3395 - val_loss: 1.8295\n",
      "Epoch 27/150\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 2.2879 - val_loss: 1.7665\n",
      "Epoch 28/150\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 2.2430 - val_loss: 1.7110\n",
      "Epoch 29/150\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2.2009 - val_loss: 1.6634\n",
      "Epoch 30/150\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 2.1664 - val_loss: 1.6219\n",
      "Epoch 31/150\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2.1330 - val_loss: 1.5874\n",
      "Epoch 32/150\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1057 - val_loss: 1.5576\n",
      "Epoch 33/150\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0799 - val_loss: 1.5334\n",
      "Epoch 34/150\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 2.0554 - val_loss: 1.5130\n",
      "Epoch 35/150\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 2.0340 - val_loss: 1.4958\n",
      "Epoch 36/150\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 2.0133 - val_loss: 1.4820\n",
      "Epoch 37/150\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.9948 - val_loss: 1.4696\n",
      "Epoch 38/150\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.9776 - val_loss: 1.4573\n",
      "Epoch 39/150\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.9595 - val_loss: 1.4464\n",
      "Epoch 40/150\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 1.9420 - val_loss: 1.4380\n",
      "Epoch 41/150\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.9263 - val_loss: 1.4280\n",
      "Epoch 42/150\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.9094 - val_loss: 1.4197\n",
      "Epoch 43/150\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.8952 - val_loss: 1.4123\n",
      "Epoch 44/150\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 1.8788 - val_loss: 1.4071\n",
      "Epoch 45/150\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.8629 - val_loss: 1.4032\n",
      "Epoch 46/150\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.8495 - val_loss: 1.3953\n",
      "Epoch 47/150\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.8354 - val_loss: 1.3867\n",
      "Epoch 48/150\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.8202 - val_loss: 1.3845\n",
      "Epoch 49/150\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.8045 - val_loss: 1.3836\n",
      "Epoch 50/150\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 1.7930 - val_loss: 1.3797\n",
      "Epoch 51/150\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.7757 - val_loss: 1.3660\n",
      "Epoch 52/150\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.7642 - val_loss: 1.3632\n",
      "Epoch 53/150\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.7472 - val_loss: 1.3714\n",
      "Epoch 54/150\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.7359 - val_loss: 1.3678\n",
      "Epoch 55/150\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 1.7236 - val_loss: 1.3509\n",
      "Epoch 56/150\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 1.7106 - val_loss: 1.3503\n",
      "Epoch 57/150\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.6970 - val_loss: 1.3570\n",
      "Epoch 58/150\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.6877 - val_loss: 1.3575\n",
      "Epoch 59/150\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.6766 - val_loss: 1.3370\n",
      "Epoch 60/150\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.6630 - val_loss: 1.3559\n",
      "Epoch 61/150\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.6519 - val_loss: 1.3623\n",
      "Epoch 62/150\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.6383 - val_loss: 1.3399\n",
      "Epoch 63/150\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.6386 - val_loss: 1.3391\n",
      "Epoch 64/150\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.6220 - val_loss: 1.3677\n",
      "Epoch 65/150\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.6164 - val_loss: 1.3353\n",
      "Epoch 66/150\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.5964 - val_loss: 1.3202\n",
      "Epoch 67/150\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.5904 - val_loss: 1.3330\n",
      "Epoch 68/150\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 1.5749 - val_loss: 1.3486\n",
      "Epoch 69/150\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.5700 - val_loss: 1.3402\n",
      "Epoch 70/150\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.5555 - val_loss: 1.3284\n",
      "Epoch 71/150\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.5473 - val_loss: 1.3265\n",
      "Epoch 72/150\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.5373 - val_loss: 1.3351\n",
      "Epoch 73/150\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.5278 - val_loss: 1.3398\n",
      "Epoch 74/150\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.5190 - val_loss: 1.3291\n",
      "Epoch 75/150\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.5103 - val_loss: 1.3273\n",
      "Epoch 76/150\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.5000 - val_loss: 1.3358\n",
      "Epoch 77/150\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.4924 - val_loss: 1.3324\n",
      "Epoch 78/150\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.4829 - val_loss: 1.3262\n",
      "Epoch 79/150\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.4751 - val_loss: 1.3341\n",
      "Epoch 80/150\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.4669 - val_loss: 1.3331\n",
      "Epoch 81/150\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.4586 - val_loss: 1.3280\n",
      "Epoch 82/150\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.4502 - val_loss: 1.3343\n",
      "Epoch 83/150\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.4427 - val_loss: 1.3288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/150\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.4348 - val_loss: 1.3300\n",
      "Epoch 85/150\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 1.4283 - val_loss: 1.3270\n",
      "Epoch 86/150\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.4203 - val_loss: 1.3279\n",
      "Epoch 87/150\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.4132 - val_loss: 1.3321\n",
      "Epoch 88/150\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.4057 - val_loss: 1.3237\n",
      "Epoch 89/150\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.3989 - val_loss: 1.3309\n",
      "Epoch 90/150\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.3930 - val_loss: 1.3252\n",
      "Epoch 91/150\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.3851 - val_loss: 1.3209\n",
      "Epoch 92/150\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 1.3790 - val_loss: 1.3280\n",
      "Epoch 93/150\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.3713 - val_loss: 1.3184\n",
      "Epoch 94/150\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.3657 - val_loss: 1.3263\n",
      "Epoch 95/150\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.3587 - val_loss: 1.3262\n",
      "Epoch 96/150\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.3525 - val_loss: 1.3200\n",
      "Epoch 97/150\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.3453 - val_loss: 1.3273\n",
      "Epoch 98/150\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.3400 - val_loss: 1.3193\n",
      "Epoch 99/150\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.3332 - val_loss: 1.3262\n",
      "Epoch 100/150\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.3267 - val_loss: 1.3217\n",
      "Epoch 101/150\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.3203 - val_loss: 1.3211\n",
      "Epoch 102/150\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.3142 - val_loss: 1.3229\n",
      "Epoch 103/150\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.3081 - val_loss: 1.3162\n",
      "Epoch 104/150\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.3023 - val_loss: 1.3213\n",
      "Epoch 105/150\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.2960 - val_loss: 1.3174\n",
      "Epoch 106/150\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.2902 - val_loss: 1.3202\n",
      "Epoch 107/150\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.2838 - val_loss: 1.3219\n",
      "Epoch 108/150\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.2786 - val_loss: 1.3156\n",
      "Epoch 109/150\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.2725 - val_loss: 1.3237\n",
      "Epoch 110/150\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.2668 - val_loss: 1.3164\n",
      "Epoch 111/150\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.2610 - val_loss: 1.3242\n",
      "Epoch 112/150\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.2556 - val_loss: 1.3131\n",
      "Epoch 113/150\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.2496 - val_loss: 1.3197\n",
      "Epoch 114/150\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.2443 - val_loss: 1.3126\n",
      "Epoch 115/150\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.2384 - val_loss: 1.3214\n",
      "Epoch 116/150\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.2331 - val_loss: 1.3159\n",
      "Epoch 117/150\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.2273 - val_loss: 1.3182\n",
      "Epoch 118/150\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 1.2221 - val_loss: 1.3095\n",
      "Epoch 119/150\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.2173 - val_loss: 1.3210\n",
      "Epoch 120/150\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.2130 - val_loss: 1.3043\n",
      "Epoch 121/150\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.2100 - val_loss: 1.3214\n",
      "Epoch 122/150\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 1.2058 - val_loss: 1.3007\n",
      "Epoch 123/150\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.2032 - val_loss: 1.3127\n",
      "Epoch 124/150\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.1934 - val_loss: 1.3266\n",
      "Epoch 125/150\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.1894 - val_loss: 1.3091\n",
      "Epoch 126/150\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.1838 - val_loss: 1.3208\n",
      "Epoch 127/150\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.1832 - val_loss: 1.3103\n",
      "Epoch 128/150\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.1782 - val_loss: 1.3060\n",
      "Epoch 129/150\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.1705 - val_loss: 1.3204\n",
      "Epoch 130/150\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.1639 - val_loss: 1.3032\n",
      "Epoch 131/150\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.1614 - val_loss: 1.3180\n",
      "Epoch 132/150\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.1590 - val_loss: 1.3125\n",
      "Epoch 133/150\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.1499 - val_loss: 1.3011\n",
      "Epoch 134/150\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.1459 - val_loss: 1.3206\n",
      "Epoch 135/150\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.1415 - val_loss: 1.3139\n",
      "Epoch 136/150\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.1355 - val_loss: 1.3060\n",
      "Epoch 137/150\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 1.1308 - val_loss: 1.3201\n",
      "Epoch 138/150\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.1268 - val_loss: 1.3118\n",
      "Epoch 139/150\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.1209 - val_loss: 1.3102\n",
      "Epoch 140/150\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.1169 - val_loss: 1.3150\n",
      "Epoch 141/150\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.1125 - val_loss: 1.3108\n",
      "Epoch 142/150\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.1081 - val_loss: 1.3158\n",
      "Epoch 143/150\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 1.1035 - val_loss: 1.3197\n",
      "Epoch 144/150\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 1.0992 - val_loss: 1.3156\n",
      "Epoch 145/150\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.0950 - val_loss: 1.3152\n",
      "Epoch 146/150\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.0911 - val_loss: 1.3120\n",
      "Epoch 147/150\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 1.0870 - val_loss: 1.3121\n",
      "Epoch 148/150\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.0829 - val_loss: 1.3180\n",
      "Epoch 149/150\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.0778 - val_loss: 1.3105\n",
      "Epoch 150/150\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.0755 - val_loss: 1.3197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b2580f1650>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=num_epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef0f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
