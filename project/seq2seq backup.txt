import tensorflow as tf
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.models import Model
import numpy as np
import json

# Define hyperparameters
input_vocab_size = 114 # Vocabulary size for the source language
output_vocab_size = 82 # Vocabulary size for the target language
embedding_dim = 100  # Dimension of the GloVe embeddings
hidden_units = 32 # Number of units in LSTM layers
sequence_length = 37 # Maximum sequence length
batch_size = 32 # Batch size
num_epochs = 200 # Number of training epochs

glove_path = 'GloVe/glove.6B.100d.txt'  # Adjust the path to your downloaded GloVe file
embedding_matrix = {}  # Create an empty dictionary to store the embeddings

with open(glove_path, 'r', encoding='utf-8') as file:
    for line in file:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embedding_matrix[word] = coefs #(coeff == Vectors)

        
# Create an embedding matrix for the source and target languages
source_embedding_matrix = np.zeros((input_vocab_size, embedding_dim))
target_embedding_matrix = np.zeros((output_vocab_size, embedding_dim))

##load the dataset
with open('card.json', 'r') as json_file:
    dataset = json.load(json_file)
    
    
#mapping input and output sequences to integers 
#input sequence = source_tokenizer
#output sequence = target_tokenizer

index_s = 1
index_t = 3
source_tokenizer = {'<PAD>':0}
target_tokenizer = {'<PAD>':0,
                    '<start>':1,
                   '<end>':2}


for i in dataset:
    question_toks = i["question_toks"]
    # print(query_toks)
    for j in question_toks:
        # print(j)
        
        if j not in source_tokenizer:
            source_tokenizer[j] = index_s
            index_s += 1
# print((frequency))
for i in dataset:
    query_toks = i["query_toks"]
    # print(query_toks)
    for j in query_toks:
        # print(j)
        
        if j not in target_tokenizer:
            target_tokenizer[j] = index_t
            index_t += 1
            
target_tokenizer_rev = {v: k for k, v in target_tokenizer.items()}
target_tokenizer

for word, i in source_tokenizer.items():
    embedding_vector = embedding_matrix.get(word)
    if embedding_vector is not None:
        source_embedding_matrix[i] = embedding_vector

for word, i in target_tokenizer.items():
    embedding_vector = embedding_matrix.get(word)
    if embedding_vector is not None:
        target_embedding_matrix[i] = embedding_vector
        
question_tokens = [example["question_toks"] for example in dataset]
query_tokens = [example["query_toks"] for example in dataset]

encoder_input_data_nopad = [[source_tokenizer[word] for word in sentence] for sentence in question_tokens]


decoder_input_data_nopad = [[1] + [target_tokenizer[word] for word in sentence] + [2] for sentence in query_tokens]


# Pad the sequences to a consistent length
def pad_sequences(sequences, max_length):
    padded_sequences = []
    for sequence in sequences:
        if len(sequence) < max_length:
            padded_sequence = sequence + [0] * (max_length - len(sequence))
        else:
            padded_sequence = sequence[:max_length]
        padded_sequences.append(padded_sequence)
    return padded_sequences

# Pad the encoder and decoder inputs
encoder_input_data = pad_sequences(encoder_input_data_nopad, sequence_length)
decoder_input_data = pad_sequences(decoder_input_data_nopad, sequence_length)
encoder_input_data = np.array(encoder_input_data)
decoder_input_data = np.array(decoder_input_data)

#decoder_target_data
target_vocab = {
    0 : 0
}

decoder_target_data = np.zeros_like(decoder_input_data)
decoder_target_data[:, :-1] = decoder_input_data[:, 1:]
decoder_target_data[:, -1] = target_vocab[0]


# Define the encoder
encoder_inputs = tf.keras.layers.Input(shape=(sequence_length,))
encoder_embedding = Embedding(input_vocab_size, embedding_dim, weights=[source_embedding_matrix], trainable=False)(encoder_inputs)
encoder_lstm = LSTM(hidden_units, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)
encoder_states = [state_h, state_c]

# Define the decoder
decoder_inputs = tf.keras.layers.Input(shape=(sequence_length,))
decoder_embedding = Embedding(output_vocab_size, embedding_dim, weights=[target_embedding_matrix], trainable=False)(decoder_inputs)
decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = Dense(output_vocab_size, activation='softmax')
output = decoder_dense(decoder_outputs)

# Build and compile the model
model = Model([encoder_inputs, decoder_inputs], output)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Train the model
model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=num_epochs, validation_split=0.2)

model = tf.keras.Sequential()

dummy_input = np.array([ 8,  9, 10, 11,  3, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0])
_ = model(dummy_input)

##saving model
model.save('seq2seq_model.h5')

from tensorflow.keras.models import load_model
model = load_model('seq2seq_model.h5')


# Load your tokenizers, embeddings, and model as described in the previous response

# Define the start token and initial decoder input
start_token = target_tokenizer['<start>']  # Replace with your actual start token
initial_decoder_input = np.zeros((1, 1))
initial_decoder_input[0, 0] = start_token

# User input
user_question = "count the total number of accounts"
user_question_tokens = user_question.split()
user_question_sequence = [source_tokenizer.get(word, 0) for word in user_question_tokens]
user_question_sequence = pad_sequences([user_question_sequence], sequence_length)

# Initialize variables for prediction
predicted_sql_tokens = []
current_input = initial_decoder_input

while len(predicted_sql_tokens) < sequence_length:
    predictions = model.predict([user_question_sequence, current_input])
    predicted_token_index = np.argmax(predictions[0, -1, :])
    
    # Convert the token index to the corresponding word
    predicted_word = [key for key, value in target_tokenizer.items() if value == predicted_token_index][0]
    predicted_sql_tokens.append(predicted_word)
    
    # Prepare the input for the next prediction
    current_input = np.array([[predicted_token_index]])
    
    
    # Check for the end token
    if predicted_word == '<end>':  # Replace with your actual end token
        break

# Convert Predicted Tokens to SQL Query
predicted_sql_query = ' '.join(predicted_sql_tokens)

# Execute or Display the SQL Query as needed
